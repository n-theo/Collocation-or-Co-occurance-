{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6BWzdD_c9HY"
      },
      "source": [
        "**The following notebook demonstrates a possible solution for tasks 3**\n",
        "\n",
        "\n",
        "We start by importing the relevant libraries. For the NLP applications, we will use the NLK library. For summarizing results we will use pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS7GS1-8u5EV"
      },
      "source": [
        "#Imports:\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import string"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VuHYD9udifO"
      },
      "source": [
        "For the purposes of this project, I chose to use the Gutenberg archive, as some of the texts are readily available in the NLTK library. More specifically, my corpus will be sourced from 'Emma', a novel by Jane Austen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987vQSrg11dO",
        "outputId": "48f9e4c6-9034-4d3c-c4db-7aab989b20dd"
      },
      "source": [
        "#download corpus, punctioation and stopwords\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Get the gutenberg data:\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyspPzHaeGkU"
      },
      "source": [
        "As the corpus is already tokenized, we will directly start by pre-processing. This will include stop-word removal (english in our case), punctuation removal and lemmatization. Since we will be exploring bi-grams, lemmatization will be preferred over stemming, since we want our words to make sense when exploring the rsults. \n",
        "\n",
        "As the corpus has some additional tokens that do not carry any contextual meaning, we add those to the stopwords (or punctioatin) in order to remove them at the pre-processing stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8gvomaf2PzI"
      },
      "source": [
        "#define stop words list and lemmatizer:\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M7n2LMQ-ifI"
      },
      "source": [
        "#append some additional tokens for preprocessing:\n",
        "custom_list = ['--',',\"','.\"','----']\n",
        "stop_words+=custom_list"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHuRK9EvrK9"
      },
      "source": [
        "#download corpus:\n",
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "emma_sentences = nltk.corpus.gutenberg.sents('austen-emma.txt')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A3phki7e91L"
      },
      "source": [
        "Below we define a function to pre-process sentences. The function will remove stop words and punctuation, transform words to lowercase, and lemmatize whre possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cMEkN_v1blS"
      },
      "source": [
        "def preprocess_sentence(sent):\n",
        "  '''\n",
        "  function that takes in tokenized sentence and returns\n",
        "  preprcossed format.\n",
        "  Performs stopword and punctioation removal, case\n",
        "  normalization and lemmatization\n",
        "  '''\n",
        "  new_sent = []\n",
        "  for word in sent:\n",
        "    if word not in stop_words and word not in string.punctuation:\n",
        "      word = word.lower()\n",
        "      word = lemmatizer.lemmatize(word)\n",
        "      for char in word:\n",
        "        if char in string.punctuation:\n",
        "          break\n",
        "      new_sent.append(word)\n",
        "  return new_sent"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7JDg6GQfR-u"
      },
      "source": [
        "With the pre-processing function defined, we edit our corpus and create a pandas dataframe that has a pre-processed sentence at each row. The dataframe step is not necessary but it helps us visualise the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjBTzvS6Gy4"
      },
      "source": [
        "#create pandas dataframe with preprocessed sentences:\n",
        "data = []\n",
        "for sent in emma_sentences:\n",
        "  new_sent = preprocess_sentence(sent)\n",
        "  if len(new_sent) >= 2:\n",
        "    data.append(new_sent)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['processed'] = data"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW7Z5ofcf0Jy"
      },
      "source": [
        "Before analysing the collocations, we want to break down the corpus to bi-grams. In order to do this, we first merege all the sentences into one list, and use the tools provided by NLTK in order to extract the bigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUJ0kzwy9uIa"
      },
      "source": [
        "#create a single list from all sentences\n",
        "merged_sents = [word for sent in df['processed'] for word in sent]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8rt_7LK-yBL"
      },
      "source": [
        "bigrams = nltk.collocations.BigramAssocMeasures()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AutBpFA9-zul"
      },
      "source": [
        "find_bigrams = nltk.collocations.BigramCollocationFinder.from_words(merged_sents)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaRgNI68gKun"
      },
      "source": [
        "With the bigrams extracted, we need to decide on the methods we will use to analyse collocations. We will take three different approaches and compare the results. \n",
        "\n",
        "Namely, we will start by a simple frequency count. This naive approach only takes into account the frequency of each bigram in the corpus and is not expected to give great results. However it will be used as a baseline for comparison. To extract the most frequent bigrams, we simply count and sort in decsending order. The 10 most frequent bigrams are visualised below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfJjjodz-3Zw"
      },
      "source": [
        "bigram_frequency = find_bigrams.ngram_fd.items()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cCYd9wC-5pj"
      },
      "source": [
        "#create frequency based table:\n",
        "bigram_freq_table = pd.DataFrame(list(bigram_frequency), columns=['bigram','frequency']).sort_values(by='frequency', ascending=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "8zi8HxYP-7uw",
        "outputId": "26549dfc-5ba0-4604-f3b3-4c8ec22b9da8"
      },
      "source": [
        "bigram_freq_table.head(10).reset_index(drop=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(mr, weston)</td>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(mr, elton)</td>\n",
              "      <td>374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(mr, knightley)</td>\n",
              "      <td>301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(miss, woodhouse)</td>\n",
              "      <td>173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(frank, churchill)</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(mr, woodhouse)</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(i, think)</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(every, thing)</td>\n",
              "      <td>126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(miss, fairfax)</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(i, shall)</td>\n",
              "      <td>122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               bigram  frequency\n",
              "0        (mr, weston)        420\n",
              "1         (mr, elton)        374\n",
              "2     (mr, knightley)        301\n",
              "3   (miss, woodhouse)        173\n",
              "4  (frank, churchill)        151\n",
              "5     (mr, woodhouse)        135\n",
              "6          (i, think)        132\n",
              "7      (every, thing)        126\n",
              "8     (miss, fairfax)        125\n",
              "9          (i, shall)        122"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTmnJyDBhRPI"
      },
      "source": [
        "We notice that the majority of the bigrams consist of a title and a name. In the context of a novel (our corpus) these are indeed collocations, as they might be main characters. However, we notice that the table includes bigrams such as 'I shall' that do not carry any contextual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c96cRbdLPA3q"
      },
      "source": [
        "find_bigrams.apply_freq_filter(25)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZwy0sohl2oF"
      },
      "source": [
        "A more sophisticated approach, that descirbes the likelihood of two words occuring together, while considering individual frequencies. This criterions is referred to as Pointwise Mutual Information (PMI) and is given by:\n",
        "\n",
        "$PMI(w_1,w_2) = log(\\frac{P(w_1,w_2)}{P(w_1)P(w_2)})$\n",
        "\n",
        "If any of the two words is rare on it's own, but the bigram is common, the probability of collocation is high, whereas if both of the words are common, the common bigram might just be a product of co-occurance. The PMI has some drawbacks, as for two rare words the bighram might be marked as significant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug_-CJ7BPHrs"
      },
      "source": [
        "#create PMI based table:\n",
        "bigram_pmi_table = pd.DataFrame(list(find_bigrams.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "MQI1RQnhPJsL",
        "outputId": "3e6b12b4-4f4c-41bc-867f-968c5d9c5cf1"
      },
      "source": [
        "bigram_pmi_table.head(10)#[:10]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>PMI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(maple, grove)</td>\n",
              "      <td>11.396518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(robert, martin)</td>\n",
              "      <td>9.704998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(colonel, campbell)</td>\n",
              "      <td>9.681323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(frank, churchill)</td>\n",
              "      <td>7.993862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(great, deal)</td>\n",
              "      <td>7.782759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(dare, say)</td>\n",
              "      <td>7.702622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(young, man)</td>\n",
              "      <td>7.281553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(young, lady)</td>\n",
              "      <td>7.205037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(my, dear)</td>\n",
              "      <td>7.135071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(john, knightley)</td>\n",
              "      <td>7.124386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                bigram        PMI\n",
              "0       (maple, grove)  11.396518\n",
              "1     (robert, martin)   9.704998\n",
              "2  (colonel, campbell)   9.681323\n",
              "3   (frank, churchill)   7.993862\n",
              "4        (great, deal)   7.782759\n",
              "5          (dare, say)   7.702622\n",
              "6         (young, man)   7.281553\n",
              "7        (young, lady)   7.205037\n",
              "8           (my, dear)   7.135071\n",
              "9    (john, knightley)   7.124386"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18o46-TIna6t"
      },
      "source": [
        "The 10 highest PMI scoring bigrams (shown above) seem to capture more context and collocation than the frequency based method. Although names are again present, verbs are less common. For the case of ('dare say') we can confidently assume significance due to the context of the book. We also notice that 'Maple Grove' is the The home of the Sucklings, Mrs. Elton's sister and brother-in-law.\n",
        "\n",
        "Finally, as a third statictic we will implement the well established, chi-sqared test. Here we assume indepence between words for the co-occurance case (null hypothesis). The statistic is given by:\n",
        "\n",
        "$Ï‡^2 = \\sum_{i,j}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}$ \n",
        "where E denotes the expectation between independent words, and O denotes the observed value. As an exaple, E(dare say) = Count(dare)Coun(say)/D, where D is the corpus size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-2raomiPpzr"
      },
      "source": [
        "#create xhi-squared based table\n",
        "bigram_chisq_table = pd.DataFrame(list(find_bigrams.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "C_AAXSw4P0R0",
        "outputId": "af66b50f-0268-4528-8f24-a8c0b60ab516"
      },
      "source": [
        "bigram_chisq_table.head(10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>chi-sq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(maple, grove)</td>\n",
              "      <td>83571.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(frank, churchill)</td>\n",
              "      <td>38395.113403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(robert, martin)</td>\n",
              "      <td>25851.554730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(colonel, campbell)</td>\n",
              "      <td>22964.431733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(mr, weston)</td>\n",
              "      <td>17598.137720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(mr, elton)</td>\n",
              "      <td>16322.448142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(jane, fairfax)</td>\n",
              "      <td>14111.819200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(great, deal)</td>\n",
              "      <td>14025.649105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(miss, woodhouse)</td>\n",
              "      <td>13117.607255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(young, man)</td>\n",
              "      <td>12967.796057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                bigram        chi-sq\n",
              "0       (maple, grove)  83571.000000\n",
              "1   (frank, churchill)  38395.113403\n",
              "2     (robert, martin)  25851.554730\n",
              "3  (colonel, campbell)  22964.431733\n",
              "4         (mr, weston)  17598.137720\n",
              "5          (mr, elton)  16322.448142\n",
              "6      (jane, fairfax)  14111.819200\n",
              "7        (great, deal)  14025.649105\n",
              "8    (miss, woodhouse)  13117.607255\n",
              "9         (young, man)  12967.796057"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9hNos-trhQZ"
      },
      "source": [
        "The resulting, highest scoring bigrams are similar to the PMI results. In order to visually compare all 3, we summarise our top 10 findings in the table below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42-HwxH0Y-2h"
      },
      "source": [
        "no_elements = 10\n",
        "top_freq = bigram_freq_table[:no_elements].bigram.values\n",
        "top_pmi = bigram_pmi_table[:no_elements].bigram.values\n",
        "top_chi =  bigram_chisq_table[:no_elements].bigram.values"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxK2ke-gaIsX"
      },
      "source": [
        "comparison_table = pd.DataFrame([top_freq,top_pmi,top_chi]).T\n",
        "comparison_table.columns = ['Frequency-Based','PMI','Chi-Squared']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "lSVCIfsQaTeN",
        "outputId": "a4a27132-6b20-476e-db2c-73fcda0dff6f"
      },
      "source": [
        "comparison_table"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency-Based</th>\n",
              "      <th>PMI</th>\n",
              "      <th>Chi-Squared</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(mr, weston)</td>\n",
              "      <td>(maple, grove)</td>\n",
              "      <td>(maple, grove)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(mr, elton)</td>\n",
              "      <td>(robert, martin)</td>\n",
              "      <td>(frank, churchill)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(mr, knightley)</td>\n",
              "      <td>(colonel, campbell)</td>\n",
              "      <td>(robert, martin)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(miss, woodhouse)</td>\n",
              "      <td>(frank, churchill)</td>\n",
              "      <td>(colonel, campbell)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(frank, churchill)</td>\n",
              "      <td>(great, deal)</td>\n",
              "      <td>(mr, weston)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(mr, woodhouse)</td>\n",
              "      <td>(dare, say)</td>\n",
              "      <td>(mr, elton)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(i, think)</td>\n",
              "      <td>(young, man)</td>\n",
              "      <td>(jane, fairfax)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(every, thing)</td>\n",
              "      <td>(young, lady)</td>\n",
              "      <td>(great, deal)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(miss, fairfax)</td>\n",
              "      <td>(my, dear)</td>\n",
              "      <td>(miss, woodhouse)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(i, shall)</td>\n",
              "      <td>(john, knightley)</td>\n",
              "      <td>(young, man)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Frequency-Based                  PMI          Chi-Squared\n",
              "0        (mr, weston)       (maple, grove)       (maple, grove)\n",
              "1         (mr, elton)     (robert, martin)   (frank, churchill)\n",
              "2     (mr, knightley)  (colonel, campbell)     (robert, martin)\n",
              "3   (miss, woodhouse)   (frank, churchill)  (colonel, campbell)\n",
              "4  (frank, churchill)        (great, deal)         (mr, weston)\n",
              "5     (mr, woodhouse)          (dare, say)          (mr, elton)\n",
              "6          (i, think)         (young, man)      (jane, fairfax)\n",
              "7      (every, thing)        (young, lady)        (great, deal)\n",
              "8     (miss, fairfax)           (my, dear)    (miss, woodhouse)\n",
              "9          (i, shall)    (john, knightley)         (young, man)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7feTWYksSEg"
      },
      "source": [
        "Just by a visual comparison, we immediately disregard the frequency based model. Out of the other two statistics, the PMI seems to be performing better as it has less title-related bigrams (mr weston, mr elton). It is worth noting that the frequency based approach, is prone to pronoun-based bigrams being classified as important. As this was not an issue for the other statistics, no measures were taken to mitigate the issue. However, as an additional step, we can POS tag our bigrams, and filter out combinations that start with pronouns. "
      ]
    }
  ]
}
